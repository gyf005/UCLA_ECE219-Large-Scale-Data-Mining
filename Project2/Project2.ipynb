{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Yanfeng, Garvit, Hyosang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# To suppress FutureWarnings. Reference: https://machinelearningmastery.com/how-to-fix-futurewarning-messages-in-scikit-learn/\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will focus on a subset of samples from the larger dataset\n",
    "class_0 = [\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"]\n",
    "class_1 = [\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"]\n",
    "\n",
    "# Fetch documents from API. Remove headers and footers.\n",
    "documents_0 = fetch_20newsgroups(categories=class_0, remove=(\"headers\", \"footers\"))\n",
    "documents_1 = fetch_20newsgroups(categories=class_1, remove=(\"headers\", \"footers\"))\n",
    "\n",
    "# Save data and assign labels\n",
    "X_0 = documents_0.data\n",
    "X_1 = documents_1.data\n",
    "X = X_0 + X_1\n",
    "Y = ([0] * len(X_0)) + ([1] * len(X_1))\n",
    "\n",
    "# Print shapes and examples for sanity check\n",
    "print(\"Class 0 example\")\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "print(\"Class 1 example\")\n",
    "print(X[-1])\n",
    "print(Y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sparse TF-IDF representations\n",
    "cv = CountVectorizer(stop_words='english', min_df=3)\n",
    "train_count = cv.fit_transform(X)  # use fit or fit_transform on the training set\n",
    "print(train_count.toarray().shape)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_count)\n",
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters = 2, n_init = 50, max_iter = 5000, random_state = 0)\n",
    "kmeans.fit(train_tfidf)\n",
    "\n",
    "Y_pred = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# Visualize contingency matrix using helper function\n",
    "\n",
    "def plot_mat(mat, xticklabels = None, yticklabels = None, pic_fname = None, size=(-1,-1), if_show_values = True,\n",
    "             colorbar = True, grid = 'k', xlabel = None, ylabel = None, title = None, vmin=None, vmax=None):\n",
    "    if size == (-1, -1):\n",
    "        size = (mat.shape[1] / 3, mat.shape[0] / 3)\n",
    "\n",
    "    fig = plt.figure(figsize=size)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    im = ax.pcolor(mat, cmap=plt.cm.Blues, linestyle='-', linewidth=0.5, edgecolor=grid, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar(im,fraction=0.046, pad=0.06)\n",
    "    \n",
    "    lda_num_topics = mat.shape[0]\n",
    "    nmf_num_topics = mat.shape[1]\n",
    "    yticks = np.arange(lda_num_topics)\n",
    "    xticks = np.arange(nmf_num_topics)\n",
    "    ax.set_xticks(xticks + 0.5)\n",
    "    ax.set_yticks(yticks + 0.5)\n",
    "    if xticklabels is None:\n",
    "        xticklabels = [str(i) for i in xticks]\n",
    "    if yticklabels is None:\n",
    "        yticklabels = [str(i) for i in yticks]\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "\n",
    "    ax.tick_params(labelright = True, labeltop = False)\n",
    "\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=15)\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel, fontsize=15)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=15)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    def show_values(pc, fmt=\"%d\", **kw):\n",
    "        pc.update_scalarmappable()\n",
    "        ax = pc.axes\n",
    "        for p, color, value in itertools.zip_longest(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "            x, y = p.vertices[:-2, :].mean(0)\n",
    "            if np.all(color[:3] > 0.5):\n",
    "                color = (0.0, 0.0, 0.0)\n",
    "            else:\n",
    "                color = (1.0, 1.0, 1.0)\n",
    "            ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw, fontsize=10)\n",
    "\n",
    "    if if_show_values:\n",
    "        show_values(im)\n",
    "    plt.tight_layout()\n",
    "    if pic_fname:\n",
    "        plt.savefig(pic_fname, dpi=300, transparent=True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Create and visualize contingency matrix while finding the best matching cluster-class pairs\n",
    "def print_contingency_matrix(Y, Y_pred):\n",
    "    contingency_matrix = confusion_matrix(Y, Y_pred)\n",
    "    rows, cols = linear_sum_assignment(contingency_matrix, maximize = True)\n",
    "    plot_mat(contingency_matrix[rows[:, np.newaxis], cols], xlabel = \"Cluster\", ylabel = \"Class\", xticklabels = cols, yticklabels = rows, size = (8,8))\n",
    "\n",
    "print_contingency_matrix(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report 5 clustering measures\n",
    "\n",
    "def print_clustering_metrics(Y, Y_pred, display=True):\n",
    "    homogeneity = homogeneity_score(Y, Y_pred)\n",
    "    completeness = completeness_score(Y, Y_pred)\n",
    "    v_measure = v_measure_score(Y, Y_pred)\n",
    "    adjusted_rand = adjusted_rand_score(Y, Y_pred)\n",
    "    adjusted_mutual_info = adjusted_mutual_info_score(Y, Y_pred)\n",
    "\n",
    "    if display:\n",
    "        print(\"Homogeneity: %0.5f\" % homogeneity)\n",
    "        print(\"Completeness: %0.5f\" % completeness)\n",
    "        print(\"V-measure: %0.5f\" % v_measure)\n",
    "        print(\"Adjusted rand index: %0.5f\" % adjusted_rand)\n",
    "        print(\"Adjusted mutual info score: %0.5f\" % adjusted_mutual_info)\n",
    "    \n",
    "    return homogeneity, completeness, v_measure, adjusted_rand, adjusted_mutual_info\n",
    "\n",
    "print_clustering_metrics(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot percentage of variance retained by r components from 1 to 1000\n",
    "rs = [1, 5, 10, 50, 100, 250, 500, 750, 1000]\n",
    "percentage_variance = []\n",
    "\n",
    "for r in rs:\n",
    "    svd = TruncatedSVD(n_components = r, random_state = 42)\n",
    "    svd.fit_transform(train_tfidf)  # fit model and perform dimensionality reduction\n",
    "    percentage_variance.append(np.sum(svd.explained_variance_ratio_) * 100)\n",
    "\n",
    "plt.plot(rs, percentage_variance)\n",
    "plt.xlabel(\"Number of top principle components\")\n",
    "plt.ylabel(\"Percentage of variance retained in data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct K-Means clustering with Truncated SVD-reduced data for different number of components\n",
    "rs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100, 300]\n",
    "\n",
    "homogeneity_scores = []\n",
    "completeness_scores = []\n",
    "v_measure_scores = []\n",
    "adjusted_rand_scores = []\n",
    "adjusted_mutual_info_scores = []\n",
    "\n",
    "# \"Note that you don’t need to perform SVD multiple times.\" - We can exclude features from the result of larger SVD\n",
    "svd = TruncatedSVD(n_components = 1000, random_state = 42)\n",
    "reduced_train_tfidf = svd.fit_transform(train_tfidf)\n",
    "\n",
    "# Compute K-Means clustering metrics for different values of r\n",
    "for r in rs:\n",
    "    r_components = reduced_train_tfidf[:, :r]\n",
    "    kmeans.fit(r_components)\n",
    "    Y_pred = kmeans.labels_\n",
    "\n",
    "    # Create list of clustering metrics for plotting\n",
    "    homogeneity, completeness, v_measure, rand, mutual_info = print_clustering_metrics(Y, Y_pred, display = False)\n",
    "    homogeneity_scores.append(homogeneity)\n",
    "    completeness_scores.append(completeness)\n",
    "    v_measure_scores.append(v_measure)\n",
    "    adjusted_rand_scores.append(rand)\n",
    "    adjusted_mutual_info_scores.append(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of metric scores against number of principal components\n",
    "def plot_clustering_metrics(title, rs, homogeneity_scores, completeness_scores, v_measure_scores, adjusted_rand_scores, adjusted_mutual_info_scores):\n",
    "    rs_index = list(range(len(rs)))\n",
    "\n",
    "    plt.plot(rs_index, homogeneity_scores, '-', label='Homogeneity')\n",
    "    plt.plot(rs_index, completeness_scores, '-', label='Completeness')\n",
    "    plt.plot(rs_index, v_measure_scores, '-', label = 'V-measure')\n",
    "    plt.plot(rs_index, adjusted_rand_scores, '-', label = 'Adjusted Rand Index')\n",
    "    plt.plot(rs_index, adjusted_mutual_info_scores, '-', label = 'Adjusted mutual information score')\n",
    "    plt.xticks(rs_index, rs)\n",
    "    plt.xlabel(\"Number of principal components, r\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Gather average for related question\n",
    "average_svd_homogeneity = statistics.mean(homogeneity_scores)\n",
    "average_svd_completeness = statistics.mean(completeness_scores)\n",
    "average_svd_v_measure = statistics.mean(v_measure_scores)\n",
    "average_svd_rand = statistics.mean(adjusted_rand_scores)\n",
    "average_svd_mutual_info = statistics.mean(adjusted_mutual_info_scores)\n",
    "\n",
    "# Best metrics\n",
    "best_svd_homogeneity = max(homogeneity_scores)\n",
    "best_svd_completeness = max(completeness_scores)\n",
    "best_svd_v_measure = max(v_measure_scores)\n",
    "best_svd_rand = max(adjusted_rand_scores)\n",
    "best_svd_mutual_info = max(adjusted_mutual_info_scores)\n",
    "\n",
    "# Create plots for SVD data gathered above\n",
    "plot_clustering_metrics(\"Clustering metrics for different number of SVD components\", rs, homogeneity_scores, completeness_scores, v_measure_scores, adjusted_rand_scores, adjusted_mutual_info_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct K-Means clustering with dimensionality reduction using NMF\n",
    "rs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100, 300]\n",
    "\n",
    "homogeneity_scores = []\n",
    "completeness_scores = []\n",
    "v_measure_scores = []\n",
    "adjusted_rand_scores = []\n",
    "adjusted_mutual_info_scores = []\n",
    "\n",
    "# Compute K-Means clustering metrics for different values of r\n",
    "for r in rs:\n",
    "    nmf = NMF(n_components = r, random_state = 42)\n",
    "    r_components = nmf.fit_transform(train_tfidf)\n",
    "    kmeans.fit(r_components)\n",
    "    Y_pred = kmeans.labels_\n",
    "\n",
    "    # Create list of clustering metrics for plotting\n",
    "    homogeneity, completeness, v_measure, rand, mutual_info = print_clustering_metrics(Y, Y_pred, display = False)\n",
    "    homogeneity_scores.append(homogeneity)\n",
    "    completeness_scores.append(completeness)\n",
    "    v_measure_scores.append(v_measure)\n",
    "    adjusted_rand_scores.append(rand)\n",
    "    adjusted_mutual_info_scores.append(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather average for related question\n",
    "average_nmf_homogeneity = statistics.mean(homogeneity_scores)\n",
    "average_nmf_completeness = statistics.mean(completeness_scores)\n",
    "average_nmf_v_measure = statistics.mean(v_measure_scores)\n",
    "average_nmf_rand = statistics.mean(adjusted_rand_scores)\n",
    "average_nmf_mutual_info = statistics.mean(adjusted_mutual_info_scores)\n",
    "\n",
    "# Best metrics\n",
    "best_nmf_homogeneity = max(homogeneity_scores)\n",
    "best_nmf_completeness = max(completeness_scores)\n",
    "best_nmf_v_measure = max(v_measure_scores)\n",
    "best_nmf_rand = max(adjusted_rand_scores)\n",
    "best_nmf_mutual_info = max(adjusted_mutual_info_scores)\n",
    "\n",
    "# Create plots for SVD data gathered above\n",
    "plot_clustering_metrics(\"Clustering metrics for different number of NMF components\", rs, homogeneity_scores, completeness_scores, v_measure_scores, adjusted_rand_scores, adjusted_mutual_info_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average scores for each reduction method, r = 1 - 300\n",
    "print(\"Average SVD\")\n",
    "print(average_svd_homogeneity)\n",
    "print(average_svd_completeness)\n",
    "print(average_svd_v_measure)\n",
    "print(average_svd_rand)\n",
    "print(average_svd_mutual_info)\n",
    "\n",
    "print(\"Average NMF\")\n",
    "print(average_nmf_homogeneity)\n",
    "print(average_nmf_completeness)\n",
    "print(average_nmf_v_measure)\n",
    "print(average_nmf_rand)\n",
    "print(average_nmf_mutual_info)\n",
    "\n",
    "# Best scores for each reduction method\n",
    "print(\"Best SVD\")\n",
    "print(best_svd_homogeneity)\n",
    "print(best_svd_completeness)\n",
    "print(best_svd_v_measure)\n",
    "print(best_svd_rand)\n",
    "print(best_svd_mutual_info)\n",
    "\n",
    "print(\"Best NMF\")\n",
    "print(best_nmf_homogeneity)\n",
    "print(best_nmf_completeness)\n",
    "print(best_nmf_v_measure)\n",
    "print(best_nmf_rand)\n",
    "print(best_nmf_mutual_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal choices based on analysis of graphs\n",
    "optimal_svd_r = 9\n",
    "optimal_nmf_r = 2\n",
    "\n",
    "# Helper to create the plot\n",
    "def create_cluster_plot(title, reduced_matrix, Y, use_clustering_labels = False):\n",
    "    kmeans.fit(reduced_matrix)\n",
    "    if use_clustering_labels:\n",
    "        Y_pred = kmeans.labels_\n",
    "    else:\n",
    "        Y_pred = Y\n",
    "    plt.scatter(reduced_matrix[:,0], reduced_matrix[:,1], c = Y_pred)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster plots for optimal SVD\n",
    "optimal_svd = TruncatedSVD(n_components = optimal_svd_r, random_state = 42)\n",
    "reduced_train_tfidf = optimal_svd.fit_transform(train_tfidf)\n",
    "\n",
    "# Projecting to 2D plane\n",
    "two_d_svd = TruncatedSVD(n_components = 2, random_state = 42)\n",
    "projected_train_tfidf = two_d_svd.fit_transform(reduced_train_tfidf)\n",
    "\n",
    "# Plot for ground truth\n",
    "create_cluster_plot(\"SVD visualization by ground truth class label\", projected_train_tfidf, Y)\n",
    "\n",
    "# Plot for clustering label\n",
    "create_cluster_plot(\"SVD visualization by clustering label\", projected_train_tfidf, Y, use_clustering_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster plots for optimal NMF\n",
    "optimal_nmf = NMF(n_components = optimal_nmf_r, random_state = 42)\n",
    "reduced_train_tfidf = optimal_nmf.fit_transform(train_tfidf)\n",
    "\n",
    "# No need to project to 2D space, as we already use n_components = 2\n",
    "# Plot for ground truth\n",
    "create_cluster_plot(\"NMF visualization by ground truth class label\", reduced_train_tfidf, Y)\n",
    "\n",
    "# Plot for clustering label\n",
    "create_cluster_plot(\"NMF visualization by clustering label\", reduced_train_tfidf, Y, use_clustering_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering of entire 20 classes\n",
    "documents = fetch_20newsgroups(remove=(\"headers\", \"footers\"))\n",
    "\n",
    "# Save data and assign labels\n",
    "X = documents.data\n",
    "Y = documents.target\n",
    "classes = documents.target_names\n",
    "\n",
    "# Print examples\n",
    "print(X[-1])\n",
    "print(Y[-1])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sparse TF-IDF representations\n",
    "cv = CountVectorizer(stop_words='english', min_df=3)\n",
    "train_count = cv.fit_transform(X)  # use fit or fit_transform on the training set\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we get baseline results by using the entire TF-IDF representation without reduction\n",
    "kmeans = KMeans(n_clusters = 20, n_init = 50, max_iter = 5000, random_state=0)\n",
    "\n",
    "kmeans.fit(train_tfidf)\n",
    "Y_pred = kmeans.labels_\n",
    "\n",
    "# Print clustering metrics and contingency matrix\n",
    "print_clustering_metrics(Y, Y_pred)\n",
    "print_contingency_matrix(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 20, n_init = 50, max_iter = 5000, random_state=0)\n",
    "\n",
    "# Conduct K-Means clustering with Truncated SVD-reduced data for different number of components\n",
    "rs = [5, 20, 200]\n",
    "\n",
    "homogeneity_scores = []\n",
    "completeness_scores = []\n",
    "v_measure_scores = []\n",
    "adjusted_rand_scores = []\n",
    "adjusted_mutual_info_scores = []\n",
    "\n",
    "# \"Note that you don’t need to perform SVD multiple times.\" - We can exclude features from the result of larger (> 300) SVD\n",
    "svd = TruncatedSVD(n_components = 500, random_state=42)\n",
    "reduced_train_tfidf = svd.fit_transform(train_tfidf)\n",
    "\n",
    "# Compute K-Means clustering metrics for different values of r\n",
    "for r in rs:\n",
    "    r_components = reduced_train_tfidf[:, :r]\n",
    "    kmeans.fit(r_components)\n",
    "    Y_pred = kmeans.labels_\n",
    "\n",
    "    # Create list of clustering metrics for plotting\n",
    "    homogeneity, completeness, v_measure, rand, mutual_info = print_clustering_metrics(Y, Y_pred, display = False)\n",
    "    homogeneity_scores.append(homogeneity)\n",
    "    completeness_scores.append(completeness)\n",
    "    v_measure_scores.append(v_measure)\n",
    "    adjusted_rand_scores.append(rand)\n",
    "    adjusted_mutual_info_scores.append(mutual_info)\n",
    "    print(\"r = {0} completed\".format(r))\n",
    "\n",
    "# Gather average for related question\n",
    "average_svd_homogeneity = statistics.mean(homogeneity_scores)\n",
    "average_svd_completeness = statistics.mean(completeness_scores)\n",
    "average_svd_v_measure = statistics.mean(v_measure_scores)\n",
    "average_svd_rand = statistics.mean(adjusted_rand_scores)\n",
    "average_svd_mutual_info = statistics.mean(adjusted_mutual_info_scores)\n",
    "\n",
    "# Create plots for SVD data gathered above\n",
    "plot_clustering_metrics(\"Clustering metrics for different number of SVD components\", \n",
    "                        rs, homogeneity_scores, completeness_scores, v_measure_scores, \n",
    "                        adjusted_rand_scores, adjusted_mutual_info_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "train_SVD = svd.fit_transform(train_tfidf)\n",
    "print(\"Reduced the dimensionalities\")\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans_SVD = KMeans(n_clusters = 20, n_init = 50, max_iter = 5000, random_state = 0).fit(train_SVD)\n",
    "\n",
    "print(\"Computed K-Means\")\n",
    "\n",
    "# Visualize the contingency matrix\n",
    "print_contingency_matrix(Y, kmeans_SVD.labels_)\n",
    "print_clustering_metrics(Y, kmeans_SVD.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 20, n_init = 50, max_iter = 5000, random_state=0)\n",
    "\n",
    "# Conduct K-Means clustering with dimensionality reduction using NMF\n",
    "rs = [5, 20, 200]\n",
    "\n",
    "homogeneity_scores = []\n",
    "completeness_scores = []\n",
    "v_measure_scores = []\n",
    "adjusted_rand_scores = []\n",
    "adjusted_mutual_info_scores = []\n",
    "\n",
    "# Compute K-Means clustering metrics for different values of r\n",
    "for r in rs:\n",
    "    nmf = NMF(n_components = r, random_state=42)\n",
    "    r_components = nmf.fit_transform(train_tfidf)\n",
    "    kmeans.fit(r_components)\n",
    "    Y_pred = kmeans.labels_\n",
    "\n",
    "    # Create list of clustering metrics for plotting\n",
    "    homogeneity, completeness, v_measure, rand, mutual_info = print_clustering_metrics(Y, Y_pred, display = False)\n",
    "    homogeneity_scores.append(homogeneity)\n",
    "    completeness_scores.append(completeness)\n",
    "    v_measure_scores.append(v_measure)\n",
    "    adjusted_rand_scores.append(rand)\n",
    "    adjusted_mutual_info_scores.append(mutual_info)\n",
    "\n",
    "    print(\"r = {0} completed\".format(r))\n",
    "\n",
    "# Gather average for related question\n",
    "average_nmf_homogeneity = statistics.mean(homogeneity_scores)\n",
    "average_nmf_completeness = statistics.mean(completeness_scores)\n",
    "average_nmf_v_measure = statistics.mean(v_measure_scores)\n",
    "average_nmf_rand = statistics.mean(adjusted_rand_scores)\n",
    "average_nmf_mutual_info = statistics.mean(adjusted_mutual_info_scores)\n",
    "\n",
    "# Create plots for SVD data gathered above\n",
    "plot_clustering_metrics(\"Clustering metrics for different number of NMF components\", \n",
    "                        rs, homogeneity_scores, completeness_scores, v_measure_scores, \n",
    "                        adjusted_rand_scores, adjusted_mutual_info_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the same procedure with NMF transformation\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=20, random_state=42)\n",
    "train_NMF = nmf.fit_transform(train_tfidf)\n",
    "print(\"Reduced the dimensionalities\")\n",
    "\n",
    "kmeans_NMF = KMeans(n_clusters = 20, n_init = 50, max_iter = 5000, random_state = 0).fit(train_NMF)\n",
    "print(\"Computed K-Means\")\n",
    "\n",
    "print_contingency_matrix(Y, kmeans_NMF.labels_)\n",
    "print_clustering_metrics(Y, kmeans_NMF.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11, 12, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP to reduce the dimensionality\n",
    "import umap.umap_ as umap\n",
    "\n",
    "kmeans_UMAP = KMeans(n_clusters = 20, n_init = 50, max_iter = 5000, random_state = 0)\n",
    "\n",
    "# Store predictions from each of the hyperparameter combinations for later analysis\n",
    "y_preds = []\n",
    "\n",
    "# For each of the two metrics and three n_components values, run UMAP reduce and get clustering results\n",
    "for metric in ['euclidean', 'cosine']:\n",
    "    for n_components in [5, 20, 200]:\n",
    "        umap_reduce = umap.UMAP(n_components=n_components, metric=metric, random_state = 42).fit_transform(train_tfidf)\n",
    "        kmeans_UMAP.fit(umap_reduce)\n",
    "        # Store clustering result for analysis\n",
    "        y_preds.append(kmeans_UMAP.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean, n_components = 5\n",
    "print_contingency_matrix(Y, y_preds[0])\n",
    "print_clustering_metrics(Y, y_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean, n_components = 20\n",
    "print_contingency_matrix(Y, y_preds[1])\n",
    "print_clustering_metrics(Y, y_preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean, n_components = 200\n",
    "print_contingency_matrix(Y, y_preds[2])\n",
    "print_clustering_metrics(Y, y_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine, n_components = 5\n",
    "print_contingency_matrix(Y, y_preds[3])\n",
    "print_clustering_metrics(Y, y_preds[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine, n_components = 20\n",
    "print_contingency_matrix(Y, y_preds[4])\n",
    "print_clustering_metrics(Y, y_preds[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine, n_components = 200\n",
    "print_contingency_matrix(Y, y_preds[5])\n",
    "print_clustering_metrics(Y, y_preds[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "import umap.plot\n",
    "\n",
    "# Get similar plot for UMAP (as for SVD and NMF in Q8)\n",
    "class_0 = [\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"]\n",
    "class_1 = [\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"]\n",
    "\n",
    "# Fetch documents from API. Remove headers and footers.\n",
    "documents_0 = fetch_20newsgroups(categories=class_0, remove=(\"headers\", \"footers\"))\n",
    "documents_1 = fetch_20newsgroups(categories=class_1, remove=(\"headers\", \"footers\"))\n",
    "\n",
    "# Save data and assign labels\n",
    "X_0 = documents_0.data\n",
    "X_1 = documents_1.data\n",
    "X = X_0 + X_1\n",
    "Y = ([0] * len(X_0)) + ([1] * len(X_1))\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df=3)\n",
    "train_count = cv.fit_transform(X)  # use fit or fit_transform on the training set\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_count)\n",
    "\n",
    "# Run UMAP reduce and get clustering results\n",
    "umap_reduce = umap.UMAP(n_components=2, metric='cosine', random_state = 42).fit(train_tfidf)\n",
    "\n",
    "# Plot UMAP data points as labelled by ground truth\n",
    "umap.plot.points(umap_reduce, labels=np.array(Y))\n",
    "plt.title('UMAP visualization by ground truth class label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering of entire 20 classes\n",
    "documents = fetch_20newsgroups(remove=(\"headers\", \"footers\"))\n",
    "\n",
    "# Save data and assign labels\n",
    "X = documents.data\n",
    "Y = documents.target\n",
    "classes = documents.target_names\n",
    "\n",
    "# Print examples\n",
    "print(X[-1])\n",
    "print(Y[-1])\n",
    "print(classes)\n",
    "\n",
    "# Generate sparse TF-IDF representations\n",
    "cv = CountVectorizer(stop_words='english', min_df=3)\n",
    "train_count = cv.fit_transform(X)  # use fit or fit_transform on the training set\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP to reduce the dimensionality\n",
    "\n",
    "import umap.umap_ as umap\n",
    "import umap.plot\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "umap_reduce = umap.UMAP(n_components=200, metric='cosine', random_state=42).fit_transform(train_tfidf)\n",
    "print(umap_reduce.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ward linkage\n",
    "\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=20, affinity='euclidean', linkage='ward')\n",
    "clustering = agg_cluster.fit(umap_reduce)\n",
    "print(clustering.labels_.shape)\n",
    "print_clustering_metrics(Y, clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single linkage\n",
    "\n",
    "agg_cluster_single = AgglomerativeClustering(n_clusters=20, affinity='cosine', linkage='single')\n",
    "clustering = agg_cluster_single.fit(umap_reduce)\n",
    "print_clustering_metrics(Y, clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN\n",
    "import hdbscan\n",
    "clusterings = {}\n",
    "\n",
    "# Compare the results among min_cluster_size = 20, 100, 200\n",
    "for min_cluster_size in [20, 100, 200]:\n",
    "    c_hdbscan = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    clustering = c_hdbscan.fit(umap_reduce)\n",
    "    print(\"****** min_cluster_size = %d ******\" % min_cluster_size)\n",
    "    print_clustering_metrics(Y, clustering.labels_)\n",
    "    clusterings[min_cluster_size] = clustering\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_contingency_matrix(Y, clusterings[100].labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def writedb(db, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(db, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def readdb(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction From Dataset\n",
    "# Run this cell if clustering.pkl is empty\n",
    "\n",
    "db = {}\n",
    "\n",
    "# Clustering of entire 20 classes\n",
    "documents = fetch_20newsgroups(remove=(\"headers\", \"footers\"))\n",
    "\n",
    "# Save data and assign labels\n",
    "X = documents.data\n",
    "Y = documents.target\n",
    "classes = documents.target_names\n",
    "\n",
    "# Print examples\n",
    "print(X[-1])\n",
    "print(Y[-1])\n",
    "print(classes)\n",
    "\n",
    "# Generate sparse TF-IDF representations\n",
    "cv = CountVectorizer(stop_words='english', min_df=3)\n",
    "train_count = cv.fit_transform(X)  # use fit or fit_transform on the training set\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_count)\n",
    "\n",
    "db[\"None\"] = train_tfidf\n",
    "\n",
    "print(\"TFIDF Stored\")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "import umap.umap_ as umap\n",
    "import umap.plot\n",
    "for r in [5, 20, 200]:\n",
    "    svd = TruncatedSVD(n_components=r, random_state=42)\n",
    "    train_SVD = svd.fit_transform(train_tfidf)\n",
    "    db[\"svd_%d\" % r] = train_SVD\n",
    "    print(\"SVD (r = %d) stored\" % r)\n",
    "\n",
    "    nmf = NMF(n_components=r, random_state=42)\n",
    "    train_NMF = nmf.fit_transform(train_tfidf)\n",
    "    db[\"nmf_%d\" % r] = train_NMF\n",
    "    print(\"NMF (r = %d) stored\" % r)\n",
    "\n",
    "    umap_reduce = umap.UMAP(n_components=r, metric='cosine', random_state=42)\n",
    "    train_umap = umap_reduce.fit_transform(train_tfidf)\n",
    "    db[\"umap_%d\" % r] = train_umap\n",
    "    print(\"UMAP (r = %d) stored\" % r)\n",
    "writedb(db, 'dimensionality_reduction.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Clustering\n",
    "# Run this cell if dimensionality_reduction.pkl is empty\n",
    "\n",
    "reductions = readdb(\"dimensionality_reduction.pkl\")\n",
    "print(reductions.keys())\n",
    "\n",
    "results = {}\n",
    "\n",
    "# K-Means\n",
    "for k in [10, 20, 50]:\n",
    "    for reduction in reductions:\n",
    "        reduction_results = reductions[reduction]\n",
    "        kmeans = KMeans(n_clusters=k, n_init=50, max_iter=5000, random_state=0)\n",
    "        labels = kmeans.fit(reduction_results).labels_\n",
    "        results[\"{0}_kmeans_{1}\".format(reduction, str(k))] = labels\n",
    "        print(\"{0}: Computed clustering K-Means (k = {1})\".format(reduction, str(k)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "for reduction in reductions:\n",
    "    reduction_results = reductions[reduction]\n",
    "    agg_cluster = AgglomerativeClustering(n_clusters=20, affinity='euclidean', linkage='ward')\n",
    "    if isinstance(reduction_results, np.ndarray):\n",
    "        labels = agg_cluster.fit(reduction_results).labels_\n",
    "    else:\n",
    "        labels = agg_cluster.fit(reduction_results.toarray()).labels_\n",
    "    results[\"{0}_agg_cluster\".format(reduction)] = labels\n",
    "    print(\"{0}: Computed clustering Agglomerative Clustering\".format(reduction))\n",
    "print(\"\\n\")\n",
    "\n",
    "# HDBSCAN\n",
    "import hdbscan\n",
    "for min_cluster_size in [100, 200]:\n",
    "    for reduction in reductions:\n",
    "        reduction_results = reductions[reduction]\n",
    "        c_hdbscan = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "        labels = c_hdbscan.fit(reduction_results).labels_\n",
    "        results[\"{0}_hdbscan_{1}\".format(reduction, str(min_cluster_size))] = labels\n",
    "        print(\"{0}: Computed clustering HDBSCAN (min_cluster_size = {1})\".format(reduction, str(min_cluster_size)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(results.keys())\n",
    "writedb(results, \"clustering.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clustering metrics for each clustering method\n",
    "\n",
    "# Clustering of entire 20 classes\n",
    "documents = fetch_20newsgroups(remove=(\"headers\", \"footers\"))\n",
    "\n",
    "# Save data and assign labels\n",
    "X = documents.data\n",
    "Y = documents.target\n",
    "classes = documents.target_names\n",
    "\n",
    "results = readdb(\"clustering.pkl\")\n",
    "overall_results = np.empty([0, 6])\n",
    "\n",
    "for result in results:\n",
    "    curr_result = results[result]\n",
    "    if not isinstance(curr_result, np.ndarray):\n",
    "        curr_result = curr_result.toarray()\n",
    "    h, c, vm, ri, mis = print_clustering_metrics(Y, curr_result, display=False)\n",
    "    mis = 0 if mis < 1e-14 else mis\n",
    "    overall_results = np.vstack((overall_results, np.array([result, h, c, vm, ri, mis])))\n",
    "\n",
    "overall_results = overall_results.T\n",
    "\n",
    "metrics = [\"Homogeneity\", \"Completeness\", \"V-measure\", \"Adjusted rand index\", \"Adjusted mutual info score\"]\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "for i in range(5):\n",
    "    sorted_score = overall_results[[0, i+1]]\n",
    "    sorted_score = sorted_score[:,sorted_score[1,:].argsort()]\n",
    "    x, y = sorted_score\n",
    "    plt.figure(figsize=(5, 15), dpi=80)\n",
    "    plt.barh(x, [float(j) for j in y])\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title(metrics[i])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "39987ca6966f75618382f2ae5b08a4b2a561e189ea1b593602189bb0fc50e1ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
